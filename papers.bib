@misc{may2014algorithmimprovingnonlocalmeans,
      title={An algorithm for improving Non-Local Means operators via low-rank approximation},
      author={Victor May and Yosi Keller and Nir Sharon and Yoel Shkolnisky},
      year={2014},
      eprint={1412.2067},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1412.2067},
}

@misc{misra2025gitchameleonevaluatingaicode,
      title={GitChameleon: Evaluating AI Code Generation Against Python Library Version Incompatibilities},
      author={Diganta Misra and Nizar Islah and Victor May and Brice Rauby and Zihan Wang and Justine Gehring and Antonio Orvieto and Muawiz Chaudhary and Eilif B. Muller and Irina Rish and Samira Ebrahimi Kahou and Massimo Caccia},
      year={2025},
      eprint={2507.12367},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2507.12367},
}

@inproceedings{nakamura-etal-2025-aurora,
    title = "Aurora-{M}: Open Source Continual Pre-training for Multilingual Language and Code",
    author = "Nakamura, Taishi  and
      Mishra, Mayank  and
      Tedeschi, Simone  and
      Chai, Yekun  and
      Stillerman, Jason T.  and
      Friedrich, Felix  and
      Yadav, Prateek  and
      Laud, Tanmay  and
      Chien, Vu Minh  and
      Zhuo, Terry Yue  and
      Misra, Diganta  and
      Bogin, Ben  and
      Vu, Xuan-Son  and
      Karpinska, Marzena  and
      Dantuluri, Arnav Varma  and
      Kusa, Wojciech  and
      Furlanello, Tommaso  and
      Yokota, Rio  and
      Muennighoff, Niklas  and
      Pai, Suhas  and
      Adewumi, Tosin  and
      Laippala, Veronika  and
      Yao, Xiaozhe  and
      Junior, Adalberto Barbosa  and
      Drozd, Aleksandr  and
      Clive, Jordan  and
      Gupta, Kshitij  and
      Chen, Liangyu  and
      Sun, Qi  and
      Tsui, Ken  and
      Moustafa-Fahmy, Nour  and
      Monti, Nicolo  and
      Dang, Tai  and
      Luo, Ziyang  and
      Bui, Tien-Tung  and
      Navigli, Roberto  and
      Mehta, Virendra  and
      Blumberg, Matthew  and
      May, Victor  and
      Nguyen, Hiep  and
      Pyysalo, Sampo",
    editor = "Rambow, Owen  and
      Wanner, Leo  and
      Apidianaki, Marianna  and
      Al-Khalifa, Hend  and
      Eugenio, Barbara Di  and
      Schockaert, Steven  and
      Darwish, Kareem  and
      Agarwal, Apoorv",
    booktitle = "Proceedings of the 31st International Conference on Computational Linguistics: Industry Track",
    month = jan,
    year = "2025",
    address = "Abu Dhabi, UAE",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.coling-industry.56/",
    pages = "656--678",
    abstract = "Pretrained language models are integral part of AI applications, but their high computational cost for training limits accessibility. Initiatives such as Bloom and StarCoder aim to democratize access to pretrained models for collaborative community development. Despite these efforts, such models encounter challenges such as limited multilingual capabilities, risks of catastrophic forgetting during continual pretraining, and the high costs of training models from scratch, alongside the need to align with AI safety standards and regulatory frameworks. This paper presents Aurora-M, a 15B parameter multilingual open-source model trained on English, Finnish, Hindi, Japanese, Vietnamese, and code. Continually pretrained from StarCoderPlus on 435B additional tokens, Aurora-M surpasses 2T tokens in total training token count. It is the first open-source multilingual model fine-tuned on human-reviewed safety instructions, thus aligning its development not only with conventional red-teaming considerations, but also with the specific concerns articulated in the Biden-Harris Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence. We evaluate Aurora-M across a wide range of tasks and languages, showcasing its robustness against catastrophic forgetting and its superior performance in multilingual settings, particularly in safety evaluations. We open-source Aurora-M and its variants to encourage responsible open-source development of large language models at https://huggingface.co/aurora-m."
}